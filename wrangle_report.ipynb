{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle report by Agbons Ekata-Inuaghata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The objective of this project was to wrangle WeRateDogs twitter data and draw insights from the analysis. Real life data rarely comes clean, hence the need to gather, assess and clean the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The project task details were:\n",
    "1. Data wrangling which consist of data gathering, data assessment and data cleaning\n",
    "\n",
    "2. Storing, analyzing and visualizing the data\n",
    "\n",
    "3. Reporting my analysis and visualisations(see act_report.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA GATHERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data for this project was in three different formats as shown below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Image Predictions file- The tweet image predictions and breed of dog present in each tweet according to a neural network. The file (image_predictions.tsv) was hosted on udacity servers and downloaded programmatically using the Requests library and the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter API and Tweet Json File- By using the tweet IDs in the WeRateDogs Twitter archive, I queried the twitter API for each tweet's JSON data using Python's tweepy library and stored each tweet's set of JSON data in a file called tweet_json.txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSESSING THE DATA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After gathering the data, the data sets gathered were assessed both visually and programmatically to check for unclean data. Unclean data is made up of dirty data with content issues and messy data with structural issues. I basically checked for tidiness and quality issues in the three data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual assessment showed me that columns such as 'doggo', 'floofer', 'pupper','puppo' in df should be a single column named stage and the column names in image_predictions such as p1, p2 were not descriptive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatic assesment showed more issues in the data set which primarily were quality issues. The issues were separated into two groups- tidiness and quality issues as shown in the jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEANING THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firstly before cleaning the data, I had to make a copy of all three dataframes so that I can do trial and error in the copies instead of the originals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cleaning of data is made up of three steps- define, code and test. Each issue identified is defined, the coding is done to resolve the issue and testing for confirmation if issue was resolved. These three steps allows for easy understanding of the data cleaning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the twitter archive data, I had to first drop retweets as we only want original ratings that had images from instructions given. Then I changed the datatype of timestamp to datetime and made the dog names consistent ie first name capital. Also noticed error with regards the denominator which has a standard of 10. So I had to check the text to know the reasons for the errors as those cells had fractions also which was used as the denominator instead of the standard denominator of 10 and I made the corrections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the image predictions data, I found 66 duplicate values for jpg_url which means same url was added to the data multiple times. Hence, I had to drop the duplicate values and also made the column names such as p1,p2 more descriptive by changing the names. Also, the dog breeds of all the three prediction columns had both upper and lower cases for the first letter. This was rectified to make the naming consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the df_JSON data, I deleted unnecessary columns as the main columns needed were retweet_count and favorite_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before storing the data, I had to merge 'df_json_clean' and 'df_image_clean' to 'df' as I noticed there was no need for all three data set. So I joined all three data sets and saved it to 'twitter_archive_master.csv'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling can help in reducing the burden of the data analysis process. It helps in finding out the most relevant information and, thereafter, supports the data analysis process so that the lesser time is consumed in bringing out the most dependable outcomes. A good data wrangler therefore should be able to combine data gotten from multiple sources and derive insights after performing necessary cleaning on the data. I have tried replicating same skills using the python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
